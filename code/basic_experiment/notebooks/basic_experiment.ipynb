{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afcba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.fx\n",
    "from torch.fx.node import Node\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from fast_nas_adapt.src.resnet18 import ResNet18\n",
    "from fast_nas_adapt.src.module2graph import GraphInterperterWithGamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2050b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphInterperterWithGumbelSoftmaxGamma(GraphInterperterWithGamma):\n",
    "    def __init__(self, mod, /, *, gamma_shift=0.0, temperature=1.0):\n",
    "        self.gamma_shift = gamma_shift\n",
    "        self.temperature = temperature\n",
    "        super().__init__(mod)\n",
    "\n",
    "    def init_gammas(self):\n",
    "        i = 0\n",
    "        gammas = []\n",
    "        self.gammas_name = {}\n",
    "        for node in self.graph.nodes:\n",
    "            if node.op == 'call_module':\n",
    "                gammas.append(np.random.randn()+self.gamma_shift)\n",
    "                self.gammas_name[str(node)] = i# перевод в str тут для удобства. в реалньых методах это не нужно\n",
    "                i+=1                        # да и вообще, тут по идее должен быть тензор/параметр\n",
    "        self.gammas =  torch.nn.Parameter(torch.as_tensor(gammas), requires_grad = True)\n",
    "        self.discrete = False \n",
    "\n",
    "    def sample_gammas(self):\n",
    "        if self.discrete:\n",
    "            return self.gammas\n",
    "        else:\n",
    "            d = torch.distributions.RelaxedBernoulli(logits=self.gammas, temperature=self.temperature)\n",
    "            return d.rsample()\n",
    "        \n",
    "    def make_gammas_discrete(self):\n",
    "        self.gammas.data = (self.gammas.data>=0) * 1.0\n",
    "        self.gammas.requires_grad = False \n",
    "        self.discrete = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6d3397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/b1/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/b1/Library/Python/3.8/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/b1/Library/Python/3.8/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18(num_classes=10)\n",
    "\n",
    "model_path = '../fast_nas_adapt/cv_experiment/results/aggressive/model_8.ckpt'\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0885b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from fast_nas_adapt.src.cifar_data import get_dataloaders\n",
    "\n",
    "train_dl, test_dl = get_dataloaders(classes=range(10), batch_size=64, img_size=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0b1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "times = pd.read_csv(\"ResNet18HomeMeasurments.csv\", index_col=0)\n",
    "times = torch.tensor(times['mean'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f95ead",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validating:   6%|█▉                            | 10/157 [00:02<00:32,  4.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.5087593, 0.5383522510528564)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "loss = []\n",
    "accs = []\n",
    "\n",
    "for i, (X, y) in tqdm(enumerate(test_dl), 'validating', total=len(test_dl)):\n",
    "    if X.shape[0] != 64:\n",
    "        continue\n",
    "\n",
    "    logits = model(X)[0].detach()\n",
    "    loss.append(loss_fn(logits, y))\n",
    "    metric(logits.argmax(-1), y)\n",
    "\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "np.mean(loss), metric.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a874625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(m, /, *, lambd):\n",
    "    optimizer = torch.optim.Adam([m.gammas])\n",
    "    \n",
    "    for i, (X, y) in tqdm(enumerate(train_dl), 'training', total=len(train_dl)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = m(X)\n",
    "        loss = loss_fn(y_pred, y) + lambd * m.gammas.dot(times)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "def validate(m):\n",
    "    metric = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    for i, (X, y) in tqdm(enumerate(test_dl), 'validating', total=len(test_dl)):\n",
    "        if X.shape[0] != 64:\n",
    "            continue\n",
    "\n",
    "        logits = m(X).detach()\n",
    "        loss.append(loss_fn(logits, y))\n",
    "        metric(logits.argmax(-1), y)\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "    return np.mean(loss), metric.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0523e3e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   1%|▍                               | 10/782 [00:04<05:39,  2.28it/s]\n",
      "validating:   6%|█▉                            | 10/157 [00:02<00:34,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.335446 0.17329545319080353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   1%|▍                               | 10/782 [00:04<05:34,  2.31it/s]\n",
      "validating:   6%|█▉                            | 10/157 [00:02<00:34,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.3067477 0.09943182021379471\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoches = 1\n",
    "\n",
    "for lambd in [0, 1]:\n",
    "    imodel = GraphInterperterWithGumbelSoftmaxGamma(model.eval())\n",
    "\n",
    "    imodel.train()\n",
    "    for epoch in range(epoches):\n",
    "        train_epoch(imodel, lambd=lambd)\n",
    "    imodel.eval()\n",
    "\n",
    "    plt.imshow(imodel.gammas.detach().reshape(-1, 10))\n",
    "    imodel.make_gammas_discrete()\n",
    "\n",
    "    loss, accuracy = validate(imodel)\n",
    "    \n",
    "    print(lambd, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928686b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
