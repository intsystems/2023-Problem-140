{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea77fe5",
   "metadata": {},
   "source": [
    "## Код, который не относится к проблеме переведен в `raw`, чтобы не запускаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d034c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.fx\n",
    "from torch.fx.node import Node\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import module2graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a41a2",
   "metadata": {},
   "source": [
    "Тут был код, который замерял время. Сейчас он в файле"
   ]
  },
  {
   "cell_type": "raw",
   "id": "833e58e3",
   "metadata": {},
   "source": [
    "measured_time = dict() # {node_name: [t1,t2,...], ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99157cc4",
   "metadata": {},
   "source": [
    "Научимся семлпировать нужным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee662cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphInterperterWithGumbelSoftmaxGamma(module2graph.GraphInterperterWithGamma):\n",
    "    def __init__(self, mod, gamma_shift=0.0, temperature=1.0):\n",
    "        self.gamma_shift = gamma_shift\n",
    "        self.temperature = temperature\n",
    "        super().__init__(mod)\n",
    "\n",
    "    def init_gammas(self):\n",
    "        i = 0\n",
    "        gammas = []\n",
    "        self.gammas_name = {}\n",
    "        for node in self.graph.nodes:\n",
    "            if node.op == 'call_module':\n",
    "                gammas.append(np.random.randn()+self.gamma_shift)\n",
    "                self.gammas_name[str(node)] = i# перевод в str тут для удобства. в реалньых методах это не нужно\n",
    "                i+=1                        # да и вообще, тут по идее должен быть тензор/параметр\n",
    "        self.gammas =  torch.nn.Parameter(torch.as_tensor(gammas), requires_grad = True)\n",
    "        self.discrete = False \n",
    "\n",
    "    def sample_gammas(self):\n",
    "        if self.discrete:\n",
    "            return self.gammas\n",
    "        else:\n",
    "            d = torch.distributions.RelaxedBernoulli(logits=self.gammas, temperature=self.temperature)\n",
    "            return d.rsample()\n",
    "        \n",
    "    def make_gammas_discrete(self):\n",
    "        self.gammas.data = (self.gammas.data>=0) * 1.0\n",
    "        self.gammas.requires_grad = False \n",
    "        self.discrete = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90ba9f",
   "metadata": {},
   "source": [
    "Получим претрейн на n эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fceacaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/b1/Desktop/m1p/2023-Problem-140/code/basic_experiment/fast_nas_adapt/cv_experiment\n",
      "ExpConfig(img_size=33, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], mse_weight=0.1, strategy='aggressive', batch_size=128, num_epochs=8, log_dir='results/aggressive', device='cpu', lr=0.05, momentum=0.9, weight_decay=0.0005, use_scheduler=False)\n",
      "Using cache found in /Users/b1/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loaded checkpoint 0\n",
      "Loaded checkpoint 1\n",
      "Loaded checkpoint 2\n",
      "Loaded checkpoint 3\n",
      "Loaded checkpoint 4\n",
      "Loaded checkpoint 5\n",
      "Loaded checkpoint 6\n",
      "Loaded checkpoint 7\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS=8\n",
    "\n",
    "%cd fast_nas_adapt/cv_experiment\n",
    "!python3 run.py --strategy=aggressive --log_dir=results/aggressive \\\n",
    "                --num_epochs={NUM_EPOCHS} \\\n",
    "                --device=cpu --lr=5e-2 --weight_decay=5e-4 \\\n",
    "                --classes 0 1 2 3 4 5 6 7 8 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91b8c9",
   "metadata": {},
   "source": [
    "Замерим время работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02edb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ResNet18\n",
    "from data import get_dataloaders\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b862d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ExpConfig\n",
    "config = ExpConfig()\n",
    "config.img_size = 33\n",
    "config.classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "config.model_path = f'results/aggressive/model_{NUM_EPOCHS}.ckpt'\n",
    "\n",
    "config.mse_weight = 0.1\n",
    "\n",
    "config.strategy = 'aggressive'\n",
    "config.batch_size = 64\n",
    "config.num_epochs = 0\n",
    "config.log_dir = 'results/aggressive'\n",
    "config.device = 'cpu'\n",
    "\n",
    "config.lr = 5e-2\n",
    "config.momentum = 0.9\n",
    "config.weight_decay = 5e-4\n",
    "config.use_scheduler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682c250e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/b1/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/b1/Library/Python/3.8/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/b1/Library/Python/3.8/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18(num_classes=len(config.classes))\n",
    "model_state_dict = torch.load(config.model_path, map_location='cpu')\n",
    "model_state_dict = {key: model_state_dict[key] for key in model_state_dict if 'fc' not in key}\n",
    "model.model.load_state_dict(model_state_dict, strict=False)\n",
    "train_dl, test_dl = get_dataloaders(classes=config.classes, batch_size=config.batch_size,\n",
    "                                    img_size=config.img_size)\n",
    "\n",
    "trainer = Trainer(model, config, train_loader=train_dl, valid_loader=test_dl)\n",
    "trainer._load_checkpoint(config.num_epochs)\n",
    "trainer.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3ac73",
   "metadata": {},
   "source": [
    "Посмортим предсказания претрейна"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a025de0",
   "metadata": {},
   "source": [
    "accs = []\n",
    "\n",
    "for i, (x, y) in tqdm(enumerate(test_dl), desc='mesuring pretrained accuracy'):\n",
    "    pred = trainer.model.forward(x)[0]\n",
    "    y_pred = pred.argmax(axis=1)\n",
    "    acc = (y.numpy() == y_pred.numpy()).mean()\n",
    "    accs.append(acc)\n",
    "\n",
    "plt.plot(range(len(accs)), accs)\n",
    "plt.gca().set_ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3424a7",
   "metadata": {},
   "source": [
    "Замерим время"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49f1963c",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "imodel = GraphInterperterWithGumbelSoftmaxGamma(trainer.model.eval(), 1.0)\n",
    "imodel.eval()\n",
    "\n",
    "for x, y in tqdm(test_dl, desc='measuring time'):\n",
    "    imodel.forward(x)\n",
    "\n",
    "print(x.shape, imodel.forward(x).shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d634e36",
   "metadata": {},
   "source": [
    "Посчитаем статистики"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7e77db2",
   "metadata": {},
   "source": [
    "data = {\n",
    "    'node' : [],\n",
    "    'mean' : [],\n",
    "    'std'  : [],\n",
    "    'len'  : [],\n",
    "    'min'  : [],\n",
    "    'max'  : [],\n",
    "}\n",
    "\n",
    "for k, v in measured_time.items():\n",
    "    data['node'].append(k)\n",
    "    data['mean'].append(np.mean(v))\n",
    "    data['std'].append(np.std(v))\n",
    "    data['len'].append(len(v))\n",
    "    data['min'].append(np.min(v))\n",
    "    data['max'].append(np.max(v))\n",
    "\n",
    "measurments = pd.DataFrame(data)\n",
    "measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8f910",
   "metadata": {},
   "source": [
    "Посмотрим на графики врмени (ноды отсортированы по среднему). Время имеет логарифмическую шкалу!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6325910",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "sorted_measurments = measurments.sort_values(by='mean')\n",
    "\n",
    "x = range(len(sorted_measurments))\n",
    "\n",
    "\n",
    "plt.fill_between(x, sorted_measurments['mean']-2*sorted_measurments['std'], \n",
    "                 sorted_measurments['mean']+2*sorted_measurments['std'],\n",
    "                 label='m±2s', alpha=0.5, color='red')\n",
    "\n",
    "plt.fill_between(x, sorted_measurments['min'], sorted_measurments['max'], \n",
    "                 label='min-max', alpha=0.2, color='black')\n",
    "\n",
    "plt.plot(x, sorted_measurments['mean'], label='mean', color='white')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Время, сек.')\n",
    "plt.xlabel('Нода')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0d25d",
   "metadata": {},
   "source": [
    "Гиперпараметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384b99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imodel = GraphInterperterWithGumbelSoftmaxGamma(model.eval())\n",
    "\n",
    "optimizer = torch.optim.Adam([imodel.gammas])\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# lambd = 0\n",
    "# times = torch.tensor(measurments['mean'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462e2ad",
   "metadata": {},
   "source": [
    "Всё ещё аналогичная проблема :(\n",
    "\n",
    "> `ValueError: Expected input batch_size (1) to match target batch_size (64).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78706cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m imodel(X)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# + lambd * imodel.gammas.dot(times)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch_number}:')\n",
    "\n",
    "    imodel.train()\n",
    "\n",
    "    epoch_history = []\n",
    "    for i, (X, y) in tqdm(enumerate(train_dl), 'training'):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = imodel(X)\n",
    "        loss = loss_fn(y_pred, y)# + lambd * imodel.gammas.dot(times)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_history.append(loss.item())\n",
    "    avg_loss = np.mean(epoch_history)\n",
    "\n",
    "    imodel.eval()\n",
    "    \n",
    "    print('epoch trained')\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, (X, y) in tqdm(enumerate(test_dl), 'validating'):\n",
    "        if X.shape[0] != 64:\n",
    "            continue\n",
    "        running_vloss += loss_fn(imodel(X), y)\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72d493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
